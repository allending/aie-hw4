{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to LangChain v0.2.0 and LCEL: LangChain Powered RAG\n",
    "\n",
    "In the following notebook we're going to focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!\n",
    "\n",
    "In the notebook, you'll complete the following Tasks:\n",
    "\n",
    "- 🤝 Breakout Room #1:\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Initialize a Simple Chain using LCEL\n",
    "  4. Implement Naive RAG using LCEL\n",
    "  \n",
    "Let's get started!\n",
    "\n"
   ],
   "metadata": {
    "id": "47eTBHYNP4g1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🤝 Breakout Room #1"
   ],
   "metadata": {
    "id": "2ayVXHXHRE_t"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1: Installing Required Libraries\n",
    "\n",
    "One of the [key features](https://blog.langchain.dev/langchain-v02-leap-to-stability/) of LangChain v0.2.0 is the compartmentalization of the various LangChain ecosystem packages and added stability.\n",
    "\n",
    "Instead of one all encompassing Python package - LangChain has a `core` package and a number of additional supplementary packages.\n",
    "\n",
    "We'll start by grabbing all of our LangChain related packages!"
   ],
   "metadata": {
    "id": "aVHd6POM0JFN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -qU langchain langchain-core langchain-community langchain-openai"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCC2AR-Q0m0x",
    "outputId": "a9936260-6134-4294-f21c-6a40111a57ca",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:34.534906Z",
     "start_time": "2024-08-27T21:16:32.019354Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can get our Qdrant dependencies!"
   ],
   "metadata": {
    "id": "S5ELHQjQ1PYs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -qU qdrant-client"
   ],
   "metadata": {
    "id": "76XeYI9P1OXO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "20cf9b81-95bd-4ebf-ae3a-cf0f5912f3eb",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:35.607470Z",
     "start_time": "2024-08-27T21:16:34.536659Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's finally get `tiktoken` and `pymupdf` so we can leverage them later on!"
   ],
   "metadata": {
    "id": "Iesey9OGCKJx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -qU tiktoken pymupdf"
   ],
   "metadata": {
    "id": "K5qIUrFuENrS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "61571142-eb50-4eaa-ac68-4c39062a36ad",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:36.546803Z",
     "start_time": "2024-08-27T21:16:35.608564Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Set Environment Variables\n",
    "\n",
    "We'll be leveraging OpenAI's suite of APIs - so we'll set our `OPENAI_API_KEY` `env` variable here!"
   ],
   "metadata": {
    "id": "sl6wTp9C5qbY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ],
   "metadata": {
    "id": "7pKAfycq73wE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "404522d0-c907-44bc-c8f8-f72084dfbb57",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:43.885249Z",
     "start_time": "2024-08-27T21:16:36.548120Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: Initialize a Simple Chain using LCEL\n",
    "\n",
    "The first thing we'll do is familiarize ourselves with LCEL and the specific ins and outs of how we can use it!"
   ],
   "metadata": {
    "id": "Q_xp54wIA56_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LLM Orchestration Tool (LangChain)\n",
    "\n",
    "Let's dive right into [LangChain](https://www.langchain.com/)!\n",
    "\n",
    "The first thing we want to do is create an object that lets us access OpenAI's `gpt-4o` model."
   ],
   "metadata": {
    "id": "SyGdhbS6SkD1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o\")"
   ],
   "metadata": {
    "id": "3Uj6SorxMj8e",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:44.571540Z",
     "start_time": "2024-08-27T21:16:43.888358Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "####❓ Question #1:\n",
    "\n",
    "What other models could we use, and how would the above code change?\n",
    "\n",
    "> HINT: Check out [this page](https://platform.openai.com/docs/models) to find the answer!"
   ],
   "metadata": {
    "id": "HsmiieEh_Ye-"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We could use any supported OpenAI model like `gpt-4o-mini`"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompt Template"
   ],
   "metadata": {
    "id": "9nU8SlHfH41T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we'll set up a prompt template - more specifically a `ChatPromptTemplate`. This will let us build a prompt we can modify when we call our LLM!"
   ],
   "metadata": {
    "id": "dcMKLZWBVYm7"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "P989b13Pvtqh",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:44.573432Z",
     "start_time": "2024-08-27T21:16:44.572169Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"You are a legendary and mythical Wizard. You speak in riddles and make obscure and pun-filled references to exotic cheeses.\"\n",
    "human_template = \"{content}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ],
   "metadata": {
    "id": "Z770j4zPS3o5",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:44.575948Z",
     "start_time": "2024-08-27T21:16:44.574169Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Our First Chain\n",
    "\n",
    "Now we can set up our first chain!\n",
    "\n",
    "A chain is simply two components that feed directly into eachother in a sequential fashion!\n",
    "\n",
    "You'll notice that we're using the pipe operator `|` to connect our `chat_prompt` to our `llm`.\n",
    "\n",
    "This is a simplified method of creating chains and it leverages the LangChain Expression Language, or LCEL.\n",
    "\n",
    "You can read more about it [here](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel), but there a few features we should be aware of out of the box (taken directly from LangChain's documentation linked above):\n",
    "\n",
    "- **Async, Batch, and Streaming Support** Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
    "\n",
    "- **Fallbacks** The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
    "\n",
    "- **Parallelism** Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "\n",
    "In the following code cell we have two components:\n",
    "\n",
    "- `chat_prompt`, which is a formattable `ChatPromptTemplate` that contains a system message and a human message.\n",
    "- `openai_chat_model`, which is a LangChain Runnable wrapped OpenAI client.\n",
    "\n",
    "We'd like to be able to pass our own `content` (as found in our `human_template`) and then have the resulting message pair sent to our model and responded to!"
   ],
   "metadata": {
    "id": "eGku_c2VVyd_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "chain = chat_prompt | openai_chat_model"
   ],
   "metadata": {
    "id": "RcJyqOiwVt04",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:44.577967Z",
     "start_time": "2024-08-27T21:16:44.576508Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice the pattern here:\n",
    "\n",
    "We invoke our chain with the `dict` `{\"content\" : \"Hello world!\"}`.\n",
    "\n",
    "It enters our chain:\n",
    "\n",
    "`{\"content\" : \"Hello world!\"}` -> `invoke()` -> `chat_prompt`\n",
    "\n",
    "Our `chat_prompt` returns a `PromptValue`, which is the formatted prompt. We then \"pipe\" the output of our `chat_prompt` into our `llm`.\n",
    "\n",
    "`PromptValue` -> `|` -> `llm`\n",
    "\n",
    "Our `llm` then takes the list of messages and provides an output which is return as a `str`!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "QV_kHCjlL_01"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(chain.invoke({\"content\": \"Hello world!\"}))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cqr2QuMtIjn",
    "outputId": "7c8655d5-c39e-42dd-8766-c784026b62ce",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:46.327767Z",
     "start_time": "2024-08-27T21:16:44.578722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Ah, greetings, seeker of wisdom! As the wheel of fortune turns, so too do the curds of knowledge. What morsel of the universe's vast cheeseboard do you wish to nibble on today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 38, 'total_tokens': 82}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_a2ff031fb5', 'finish_reason': 'stop', 'logprobs': None} id='run-ccebe5de-f645-40f6-8fb9-634d03569f08-0' usage_metadata={'input_tokens': 38, 'output_tokens': 44, 'total_tokens': 82}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try it out with a different prompt!"
   ],
   "metadata": {
    "id": "2znL48ECNteM"
   }
  },
  {
   "cell_type": "code",
   "source": "chain.invoke({\"content\" : \"Could I please have some advice on how to become a better Python Programmer?\"})",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjiTNeYXUCAB",
    "outputId": "840e9d31-7387-48f3-cbb9-7a3495985698",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:51.246798Z",
     "start_time": "2024-08-27T21:16:46.328457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, seeker of serpentine syntax and looped logic, you ask for the grail of Pythonic prowess! Hearken to my words, for in them lies the key.\\n\\nFirst, one must find the **Gouda** practice—code daily, for consistency is the curd that forms the wheel of wisdom. In the land of variables and functions, repetition is the cheese press that shapes your skills.\\n\\nSecond, explore the **Brie**-lliant libraries that Python offers. Numpy, Pandas, and Matplotlib are but a few of the enchanted tomes that can make your code sing like a choir of Swiss bells. Delve into their depths and let them age your understanding like a fine Roquefort.\\n\\nThird, seek the fellowship of fellow coders. The forums of Stack Overflow and the repositories of GitHub are like caves of Cheddar, filled with nuggets of knowledge and wisdom. Collaborate and contribute, for in sharing, you shall find the sharpness of your own intellect.\\n\\nFourth, do not fear the stinky **Limburger** of debugging. Embrace it! For every error, every exception is a clue, a breadcrumb in the labyrinth of your own making. The more you debug, the more you learn, as the stench of failure turns into the sweet smell of success.\\n\\nLastly, remember the legend of the **Parmigiano-Reggiano** Principle: simplicity is the ultimate sophistication. Write code that is clean and readable, for in clarity lies the true magic. Complex incantations and tangled spells may dazzle, but it is the straightforward charm that truly endures.\\n\\nSo go forth, brave coder, and may your journey be as rich and rewarding as a platter of the finest cheeses!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 354, 'prompt_tokens': 50, 'total_tokens': 404}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_a2ff031fb5', 'finish_reason': 'stop', 'logprobs': None}, id='run-b91e2f13-9000-43ec-8629-f816bf61d7c0-0', usage_metadata={'input_tokens': 50, 'output_tokens': 354, 'total_tokens': 404})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice how we specifically referenced our `content` format option!\n",
    "\n",
    "Now that we have the basics set up - let's see what we mean by \"Retrieval Augmented\" Generation."
   ],
   "metadata": {
    "id": "THcMz8YAWsjP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive RAG - Manually adding context through the Prompt Template\n",
    "\n",
    "Let's look at how our model performs at a simple task - defining what LangChain is!\n",
    "\n",
    "We'll redo some of our previous work to change the `system_template` to be less...verbose."
   ],
   "metadata": {
    "id": "P7o8aXbhRAPe"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qu_Uox_pPKaf",
    "outputId": "5b93eef2-c844-4a23-cd87-94b1686a943a",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:54.853860Z",
     "start_time": "2024-08-27T21:16:51.248220Z"
    }
   },
   "source": [
    "system_template = \"You are a helpful assistant.\"\n",
    "human_template = \"{content}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | openai_chat_model\n",
    "\n",
    "print(chat_chain.invoke({\"content\" : \"Please define LangChain.\"}))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework designed for developing applications that are powered by large language models (LLMs). It is particularly useful for applications that involve the following:\\n\\n1. **Data Connection**: Integrating LLMs with various data sources, such as files, websites, or APIs, to fetch and process information.\\n2. **Interaction**: Creating complex workflows or chains of interactions between the LLM and users, other systems, or additional models.\\n\\nThe framework supports the development of applications by providing tools and abstractions that simplify the process of connecting to data and managing interactions. Some common use cases for LangChain include:\\n\\n- **Chatbots**: Building sophisticated conversational agents that can pull in data from different sources to provide more accurate and context-aware responses.\\n- **Automated Assistants**: Developing assistants that can perform tasks by interacting with various APIs and systems.\\n- **Data Processing Pipelines**: Creating workflows that involve multiple steps of data retrieval, processing, and output generation.\\n\\nLangChain abstracts many of the complexities involved in working with LLMs, allowing developers to focus more on the application logic rather than the underlying infrastructure.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 22, 'total_tokens': 249}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_a2ff031fb5', 'finish_reason': 'stop', 'logprobs': None} id='run-130020e1-7214-44e1-a3a1-4f611a7268e5-0' usage_metadata={'input_tokens': 22, 'output_tokens': 227, 'total_tokens': 249}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, that's not very good - is it!\n",
    "\n",
    "The issue at play here is that our model was not trained on the idea of \"LangChain\", and so it's left with nothing but a guess - definitely not what we want the answer to be!\n",
    "\n",
    "Let's ask another simple LangChain question!"
   ],
   "metadata": {
    "id": "18KXqGI4XbMb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(chat_chain.invoke({\"content\" : \"What is LangChain Expression Language (LECL)?\"}))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRG5LwYoXnsr",
    "outputId": "58769a17-aba2-45f8-e9d4-23e96314de54",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:16:57.979335Z",
     "start_time": "2024-08-27T21:16:54.854949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain Expression Language (LECL) is a specialized language designed to facilitate the creation and manipulation of expressions within the LangChain framework. LangChain is a framework for developing applications powered by language models, and LECL serves as a tool to streamline and enhance the process of working with these language models.\\n\\nLECL allows developers to define, evaluate, and manage expressions in a structured and efficient manner. This can include tasks such as querying data, performing calculations, or manipulating text. The language is designed to be intuitive and powerful, enabling developers to easily integrate complex logic and operations into their LangChain-based applications.\\n\\nKey features of LECL might include:\\n\\n1. **Simplicity and Intuitiveness:** LECL is designed to be easy to learn and use, making it accessible to developers with varying levels of expertise.\\n\\n2. **Flexibility:** The language supports a wide range of operations and can be adapted to various use cases within the LangChain framework.\\n\\n3. **Integration:** LECL is tightly integrated with LangChain, ensuring seamless interaction with the underlying language models and other components of the framework.\\n\\n4. **Efficiency:** LECL is optimized for performance, enabling developers to create and evaluate expressions quickly and efficiently.\\n\\nBy using LECL, developers can streamline their workflow, reduce the complexity of their code, and enhance the capabilities of their LangChain applications.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 271, 'prompt_tokens': 27, 'total_tokens': 298}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None} id='run-391950a1-cd72-4ff1-b872-3482c0153145-0' usage_metadata={'input_tokens': 27, 'output_tokens': 271, 'total_tokens': 298}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "While it provides a confident response, that response is entirely ficticious! Not a great look, OpenAI!\n",
    "\n",
    "However, let's see what happens when we rework our prompts - and we add the content from the docs to our prompt as context."
   ],
   "metadata": {
    "id": "63pr0fgYXxC3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "HUMAN_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT = \"\"\"\n",
    "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
    "\n",
    "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
    "\n",
    "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
    "\n",
    "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "\n",
    "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | openai_chat_model\n",
    "\n",
    "print(chat_chain.invoke({\"query\" : \"What is LangChain Expression Language?\", \"context\" : CONTEXT}))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fgr25HjgYHwh",
    "outputId": "7d590389-8a46-4105-fbe7-c08495dced25",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.454221Z",
     "start_time": "2024-08-27T21:16:57.980938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It provides several benefits:\\n\\n1. **Async, Batch, and Streaming Support**: Chains constructed with LCEL automatically have full sync, async, batch, and streaming support, facilitating easy prototyping and subsequent exposure as an async streaming interface.\\n2. **Fallbacks**: LCEL allows for graceful error handling by easily attaching fallbacks to any chain.\\n3. **Parallelism**: Components that can be run in parallel are automatically executed in parallel, addressing the need for efficiency in LLM applications.\\n4. **Seamless LangSmith Tracing Integration**: LCEL ensures that all steps are automatically logged to LangSmith for better observability and debuggability, which is crucial as chains become more complex.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 163, 'prompt_tokens': 274, 'total_tokens': 437}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'stop', 'logprobs': None} id='run-d1c1a92a-d92a-492a-90c7-2de2a5e4d9bf-0' usage_metadata={'input_tokens': 274, 'output_tokens': 163, 'total_tokens': 437}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "You'll notice that the response is much better this time. Not only does it answer the question well - but there's no trace of confabulation (hallucination) at all!\n",
    "\n",
    "> NOTE: While RAG is an effective strategy to *help* ground LLMs, it is not nearly 100% effective. You will still need to ensure your responses are factual through some other processes\n",
    "\n",
    "That, in essence, is the idea of RAG. We provide the model with context to answer our queries - and rely on it to translate the potentially lengthy and difficult to parse context into a natural language answer!\n",
    "\n",
    "However, manually providing context is not scalable - and doesn't really offer any benefit.\n",
    "\n",
    "Enter: Retrieval Pipelines."
   ],
   "metadata": {
    "id": "ppQdtCedY7C4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task #4: Implement Naive RAG using LCEL\n",
    "\n",
    "Now we can make a naive RAG application that will help us bridge the gap between our Pythonic implementation and a fully LangChain powered solution!"
   ],
   "metadata": {
    "id": "DFmdARsVBJUq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Putting the R in RAG: Retrieval 101\n",
    "\n",
    "In order to make our RAG system useful, we need a way to provide context that is most likely to answer our user's query to the LLM as additional context.\n",
    "\n",
    "Let's tackle an immediate problem first: The Context Window.\n",
    "\n",
    "All (most) LLMs have a limited context window which is typically measured in tokens. This window is an upper bound of how much stuff we can stuff in the model's input at a time.\n",
    "\n",
    "Let's say we want to work off of a relatively large piece of source data - like the Ultimate Hitchhiker's Guide to the Galaxy. All 898 pages of it!\n",
    "\n",
    "> NOTE: It is recommended you do not run the following cells, they are purely for demonstrative purposes."
   ],
   "metadata": {
    "id": "n4AozVoEZveK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "context = \"\"\"\n",
    "EVERY HITCHHIKER'S GUIDE BOOK\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "PbXBxffibeyp",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.461383Z",
     "start_time": "2024-08-27T21:17:00.458783Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can leverage our tokenizer to count the number of tokens for us!"
   ],
   "metadata": {
    "id": "EZvgFuaXcHFT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ],
   "metadata": {
    "id": "HaKPOdSjbifn",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.733798Z",
     "start_time": "2024-08-27T21:17:00.462737Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "len(enc.encode(context))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtDiSMxpE4Xi",
    "outputId": "886fd517-9128-45ca-9fbd-5152ae7f146b",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.737315Z",
     "start_time": "2024-08-27T21:17:00.734485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "The full set comes in at a whopping *636,144* tokens.\n",
    "\n",
    "So, we have too much context. What can we do?\n",
    "\n",
    "Well, the first thing that might enter your mind is: \"Use a model with more context window\", and we could definitely do that! However, even `gpt-4-128k` wouldn't be able to fit that whole text in the context window at once.\n",
    "\n",
    "So, we can try splitting our document up into little pieces - that way, we can avoid providing too much context.\n",
    "\n",
    "We have another problem now.\n",
    "\n",
    "If we split our document up into little pieces, and we can't put all of them in the prompt. How do we decide which to include in the prompt?!\n",
    "\n",
    "> NOTE: Content splitting/chunking strategies are an active area of research and iterative developement. There is no \"one size fits all\" approach to chunking/splitting at this moment. Use your best judgement to determine chunking strategies!\n",
    "\n",
    "In order to conceptualize the following processes - let's create a toy context set!"
   ],
   "metadata": {
    "id": "5oUuZpAicLdm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TextSplitting aka Chunking\n",
    "\n",
    "We'll use the `RecursiveCharacterTextSplitter` to create our toy example.\n",
    "\n",
    "It will split based on the following rules:\n",
    "\n",
    "- Each chunk has a maximum size of 100 tokens\n",
    "- It will try and split first on the `\\n\\n` character, then on the `\\n`, then on the `<SPACE>` character, and finally it will split on individual tokens.\n",
    "\n",
    "Let's implement it and see the results!"
   ],
   "metadata": {
    "id": "UPCiOPwUfbqn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")"
   ],
   "metadata": {
    "id": "nLW9AfDKfVHn",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.743910Z",
     "start_time": "2024-08-27T21:17:00.737913Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "chunks = text_splitter.split_text(CONTEXT)"
   ],
   "metadata": {
    "id": "nPYPBe2ngT9N",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.746927Z",
     "start_time": "2024-08-27T21:17:00.744402Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "len(chunks)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_RGVlTihaQX",
    "outputId": "d1b260d2-8559-4ccc-d333-9e66543d9ed2",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.749533Z",
     "start_time": "2024-08-27T21:17:00.747588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "for chunk in chunks:\n",
    "  print(chunk)\n",
    "  print(\"----\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTYny2xchS_Z",
    "outputId": "35a390f4-2e17-46f6-b4d2-7c9c86773507",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.751711Z",
     "start_time": "2024-08-27T21:17:00.750138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "----\n",
      "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
      "\n",
      "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
      "----\n",
      "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "As is shown in our result, we've split each section into 100 token chunks - cleanly separated by `\\n\\n` characters!"
   ],
   "metadata": {
    "id": "98hOgu5Yhefv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####🏗️ Activity #1:\n",
    "\n",
    "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
    "\n",
    "Brainstorm some ideas that would split large single documents into smaller documents.\n",
    "\n",
    "1. We could split a large document per page (with some overlap between pages)\n",
    "2. If the context window was large enough, and in a document that had section metadata, we could split it by section-by-section basis \n",
    "3. For a document like a textbook, perhaps preprocessing it by traversing the index of the book and only indexing paragraphs/pages reference by the book index"
   ],
   "metadata": {
    "id": "3PTiJ2utMpqq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings and Dense Vector Search\n",
    "\n",
    "Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.\n",
    "\n",
    "This sounds like a perfect job for embeddings!\n",
    "\n",
    "We'll be using OpenAI's `text-embedding-3` model as our embedding model today!\n",
    "\n",
    "Let's load it up through LangChain."
   ],
   "metadata": {
    "id": "rj18Rjafhp7d"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ],
   "metadata": {
    "id": "quNjOLWspOVN",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:00.782994Z",
     "start_time": "2024-08-27T21:17:00.752343Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "####❓ Question #2:\n",
    "\n",
    "What is the embedding dimension, given that we're using `text-embedding-3-small`?\n",
    "\n",
    "> HINT: Check out the [docs](https://platform.openai.com/docs/guides/embeddings) to help you answer this question."
   ],
   "metadata": {
    "id": "dsGZ92hm9IeX"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The default embedding dimension is 1536"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding the Embeddings for Our Chunks\n",
    "\n",
    "First, let's find all our embeddings for each chunk and store them in a convenient format for later."
   ],
   "metadata": {
    "id": "ByK-zb0FsnqR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "for chunk in chunks:\n",
    "  embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
   ],
   "metadata": {
    "id": "ZHl-u6Fxske9",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:02.380430Z",
     "start_time": "2024-08-27T21:17:00.783745Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "for k,v in embeddings_dict.items():\n",
    "  print(f\"Chunk - {k}\")\n",
    "  print(\"---\")\n",
    "  print(f\"Embedding - Vector of Size: {len(v)}\")\n",
    "  print(\"\\n\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhJ9wY2etK7y",
    "outputId": "bc3554eb-9075-4a7f-c028-7dae06e7256c",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:02.387110Z",
     "start_time": "2024-08-27T21:17:02.382217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk - LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n",
      "Chunk - Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
      "\n",
      "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n",
      "Chunk - Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
      "---\n",
      "Embedding - Vector of Size: 1536\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, great. Let's create a query - and then embed it!"
   ],
   "metadata": {
    "id": "SOxYybdNtkWv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Can LCEL help take code from the notebook to production?\"\n",
    "\n",
    "query_vector = embedding_model.embed_query(query)\n",
    "print(f\"Vector of Size: {len(query_vector)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQD5Zwl1tLrZ",
    "outputId": "d42d7488-edca-458b-dca4-1b11f3536cb2",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:02.704990Z",
     "start_time": "2024-08-27T21:17:02.388946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector of Size: 1536\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's compare it against each existing chunk's embedding by using cosine similarity."
   ],
   "metadata": {
    "id": "kkJhyvgEt5Vt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec_1, vec_2):\n",
    "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
   ],
   "metadata": {
    "id": "vfyDZlpmfWYa",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:02.709423Z",
     "start_time": "2024-08-27T21:17:02.706392Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "max_similarity = -float('inf')\n",
    "closest_chunk = \"\"\n",
    "\n",
    "for chunk, chunk_vector in embeddings_dict.items():\n",
    "  cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
    "\n",
    "  if cosine_similarity_score > max_similarity:\n",
    "    closest_chunk = chunk\n",
    "    max_similarity = cosine_similarity_score\n",
    "\n",
    "print(closest_chunk)\n",
    "print(max_similarity)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwnJ0uYQt-G_",
    "outputId": "5ed2d13c-564c-4e2d-b06a-9f4e2e75d9d0",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:02.715187Z",
     "start_time": "2024-08-27T21:17:02.710868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
      "\n",
      "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
      "0.5372984870519116\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we get the expected result, which is the passage that specifically mentions prototyping in a Jupyter Notebook!"
   ],
   "metadata": {
    "id": "JDC7HS7iumN-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a Retriever\n",
    "\n",
    "Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!\n",
    "\n",
    "First, we'll wrap the above in a helper function!"
   ],
   "metadata": {
    "id": "UPJexL1kuw45"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def retrieve_context(query, embeddings_dict, embedding_model):\n",
    "  query_vector = embedding_model.embed_query(query)\n",
    "  max_similarity = -float('inf')\n",
    "  closest_chunk = \"\"\n",
    "\n",
    "  for chunk, chunk_vector in embeddings_dict.items():\n",
    "    cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
    "\n",
    "    if cosine_similarity_score > max_similarity:\n",
    "      closest_chunk = chunk\n",
    "      max_similarity = cosine_similarity_score\n",
    "\n",
    "  return closest_chunk"
   ],
   "metadata": {
    "id": "tnLpo26pu8-1",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:02.718629Z",
     "start_time": "2024-08-27T21:17:02.716226Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's add it to our pipeline!"
   ],
   "metadata": {
    "id": "ytrINkVrvL1Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def simple_rag(query, embeddings_dict, embedding_model, chat_chain):\n",
    "  context = retrieve_context(query, embeddings_dict, embedding_model)\n",
    "\n",
    "  response = chat_chain.invoke({\"query\" : query, \"context\" : context})\n",
    "\n",
    "  return_package = {\n",
    "      \"query\" : query,\n",
    "      \"response\" : response,\n",
    "      \"retriever_context\" : context\n",
    "  }\n",
    "\n",
    "  return return_package"
   ],
   "metadata": {
    "id": "Q26pl1osvNmL",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:02.721878Z",
     "start_time": "2024-08-27T21:17:02.719555Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "simple_rag(\"Can LCEL help take code from the notebook to production?\", embeddings_dict, embedding_model, chat_chain)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHjbWxTAvtLS",
    "outputId": "86e4baf5-a3e4-44c7-ba85-19f7a1d852ba",
    "ExecuteTime": {
     "end_time": "2024-08-27T21:17:04.603227Z",
     "start_time": "2024-08-27T21:17:02.722829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Can LCEL help take code from the notebook to production?',\n",
       " 'response': AIMessage(content='Yes, LCEL can help take code from the notebook to production. Since any chain constructed using LCEL will automatically have full sync, async, batch, and streaming support, it makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface for production use.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 152, 'total_tokens': 218}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'stop', 'logprobs': None}, id='run-2a2f74be-f7ce-4a67-8fde-0d4d8a23f2f4-0', usage_metadata={'input_tokens': 152, 'output_tokens': 66, 'total_tokens': 218}),\n",
       " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "####❓ Question #3:\n",
    "\n",
    "What does LCEL do that makes it more reliable at scale?\n",
    "\n",
    "> HINT: Use your newly created `simple_rag` to help you answer this question!"
   ],
   "metadata": {
    "id": "cD2URVX3O3XJ"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T21:29:23.017390Z",
     "start_time": "2024-08-27T21:29:20.541582Z"
    }
   },
   "cell_type": "code",
   "source": "simple_rag(\"What does LCEL do that makes it more reliable at scale?\", embeddings_dict, embedding_model, chat_chain)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What does LCEL do that makes it more reliable at scale?',\n",
       " 'response': AIMessage(content='LCEL makes chains more reliable at scale by providing full support for sync, async, batch, and streaming operations. This ensures that chains can be easily prototyped in a synchronous interface and then seamlessly exposed as asynchronous streaming interfaces, enhancing reliability and scalability in various operational contexts.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 153, 'total_tokens': 208}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_a2ff031fb5', 'finish_reason': 'stop', 'logprobs': None}, id='run-e96fca7f-b26d-47b1-8cc7-04dec7105082-0', usage_metadata={'input_tokens': 153, 'output_tokens': 55, 'total_tokens': 208}),\n",
       " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> LCEL makes chains more reliable at scale by providing full support for sync, async, batch, and streaming operations. This ensures that chains can be easily prototyped in a synchronous interface and then seamlessly exposed as asynchronous streaming interfaces, enhancing reliability and scalability in various operational contexts."
  }
 ]
}
